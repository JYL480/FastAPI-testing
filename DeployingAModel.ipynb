{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPNZPczTuC8/1ExRY6sLOl7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "532d4ca700fc4d54993a29635d9288c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b89b0c7c676f410d83f37b753bb42138",
              "IPY_MODEL_899ce6a929ce49169cee5e096e803911",
              "IPY_MODEL_c9835de8a1494ecf8646acf42e124ca4"
            ],
            "layout": "IPY_MODEL_39c5e2902cd54dde82230a90588969c3"
          }
        },
        "b89b0c7c676f410d83f37b753bb42138": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_758c363ce17542daa64063d6a09029e8",
            "placeholder": "​",
            "style": "IPY_MODEL_c9709f76636a4be9a2998653e5ccd63b",
            "value": "100%"
          }
        },
        "899ce6a929ce49169cee5e096e803911": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bf63608ed104ca78d72f64c14d32100",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1a7d2a37975402ea15e8a62a5c951be",
            "value": 10
          }
        },
        "c9835de8a1494ecf8646acf42e124ca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82ee1be5e6324264af5262735500d311",
            "placeholder": "​",
            "style": "IPY_MODEL_e6a2b23167d4472eb02224cccd9490a6",
            "value": " 10/10 [01:02&lt;00:00,  5.60s/it]"
          }
        },
        "39c5e2902cd54dde82230a90588969c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "758c363ce17542daa64063d6a09029e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9709f76636a4be9a2998653e5ccd63b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9bf63608ed104ca78d72f64c14d32100": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1a7d2a37975402ea15e8a62a5c951be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82ee1be5e6324264af5262735500d311": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6a2b23167d4472eb02224cccd9490a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JYL480/FoodClassificationFastAPI/blob/main/DeployingAModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0MSFcPS_dAf",
        "outputId": "912b5028-7f07-43b7-f307-30ef65be4b03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.3.0+cu121\n",
            "torchvision version: 0.18.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "# assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "# assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "print(f\"torch version: {torch.__version__}\")\n",
        "print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sN67au6_qWE",
        "outputId": "ed9a130a-eb85-4a61-c361-6cb2426d7f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 4056, done.\u001b[K\n",
            "remote: Total 4056 (delta 0), reused 0 (delta 0), pack-reused 4056\u001b[K\n",
            "Receiving objects: 100% (4056/4056), 646.90 MiB | 33.72 MiB/s, done.\n",
            "Resolving deltas: 100% (2372/2372), done.\n",
            "Updating files: 100% (248/248), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using the 20% dataset for the data loaders! and 10 epochs!"
      ],
      "metadata": {
        "id": "xQjD3TK7_yTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pizza, steak, sushi images from GitHub\n",
        "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
        "                                     destination=\"pizza_steak_sushi_20_percent\")\n",
        "\n",
        "data_20_percent_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFqqcukU_3K-",
        "outputId": "a5d88c2e-5c8e-49b9-a4a8-d61f76e611d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Did not find data/pizza_steak_sushi_20_percent directory, creating one...\n",
            "[INFO] Downloading pizza_steak_sushi_20_percent.zip from https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip...\n",
            "[INFO] Unzipping pizza_steak_sushi_20_percent.zip data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('data/pizza_steak_sushi_20_percent')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup directory paths to train and test images\n",
        "train_dir = data_20_percent_path / \"train\"\n",
        "test_dir = data_20_percent_path / \"test\""
      ],
      "metadata": {
        "id": "vfNxoQwx_5En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_effnetb2_model(num_classes:int=3,\n",
        "                          seed:int=42):\n",
        "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int, optional): number of classes in the classifier head.\n",
        "            Defaults to 3.\n",
        "        seed (int, optional): random seed value. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module): EffNetB2 feature extractor model.\n",
        "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
        "    \"\"\"\n",
        "    # 1, 2, 3. Create EffNetB2 pretrained weights, transforms and model\n",
        "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "    # 4. Freeze all layers in base model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 5. Change classifier head with random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.3, inplace=True),\n",
        "        nn.Linear(in_features=1408, out_features=num_classes),\n",
        "    )\n",
        "\n",
        "    return model, transforms"
      ],
      "metadata": {
        "id": "rmVXJqb0_-dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,\n",
        "                                                      seed=42)\n",
        "\n",
        "# Note the classifier output has already been changed in the funciton above!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRAb0MM-ABgn",
        "outputId": "17be2092-1acb-4774-bc4d-5a2671b247ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-c35c1473.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-c35c1473.pth\n",
            "100%|██████████| 35.2M/35.2M [00:00<00:00, 78.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# # Print EffNetB2 model summary (uncomment for full output)\n",
        "# summary(effnetb2,\n",
        "#         input_size=(1, 3, 224, 224),\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "CjjnMXLTACjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datra loaders things!\n",
        "\n"
      ],
      "metadata": {
        "id": "SO63EG69AJmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup DataLoaders\n",
        "from going_modular.going_modular import data_setup\n",
        "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                                 test_dir=test_dir,\n",
        "                                                                                                 transform=effnetb2_transforms,\n",
        "                                                                                                 batch_size=32)"
      ],
      "metadata": {
        "id": "385yLsG2AFM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NY_tEILRAfur",
        "outputId": "c981194b-a4fb-420d-d91c-be549e24c010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
        "                             lr=1e-3)\n",
        "# Setup loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Set seeds for reproducibility and train the model\n",
        "set_seeds()\n",
        "effnetb2_results = engine.train(model=effnetb2,\n",
        "                                train_dataloader=train_dataloader_effnetb2,\n",
        "                                test_dataloader=test_dataloader_effnetb2,\n",
        "                                epochs=10,\n",
        "                                optimizer=optimizer,\n",
        "                                loss_fn=loss_fn,\n",
        "                                device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234,
          "referenced_widgets": [
            "532d4ca700fc4d54993a29635d9288c3",
            "b89b0c7c676f410d83f37b753bb42138",
            "899ce6a929ce49169cee5e096e803911",
            "c9835de8a1494ecf8646acf42e124ca4",
            "39c5e2902cd54dde82230a90588969c3",
            "758c363ce17542daa64063d6a09029e8",
            "c9709f76636a4be9a2998653e5ccd63b",
            "9bf63608ed104ca78d72f64c14d32100",
            "c1a7d2a37975402ea15e8a62a5c951be",
            "82ee1be5e6324264af5262735500d311",
            "e6a2b23167d4472eb02224cccd9490a6"
          ]
        },
        "id": "NhpfjZJeAZD3",
        "outputId": "5f53596e-4327-40c5-de6a-0a1cfd4cd0c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "532d4ca700fc4d54993a29635d9288c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 0.9839 | train_acc: 0.5667 | test_loss: 0.7393 | test_acc: 0.9409\n",
            "Epoch: 2 | train_loss: 0.7135 | train_acc: 0.8396 | test_loss: 0.5862 | test_acc: 0.9409\n",
            "Epoch: 3 | train_loss: 0.5874 | train_acc: 0.8958 | test_loss: 0.4891 | test_acc: 0.9563\n",
            "Epoch: 4 | train_loss: 0.4488 | train_acc: 0.9146 | test_loss: 0.4338 | test_acc: 0.9409\n",
            "Epoch: 5 | train_loss: 0.4277 | train_acc: 0.9125 | test_loss: 0.3907 | test_acc: 0.9443\n",
            "Epoch: 6 | train_loss: 0.4392 | train_acc: 0.8896 | test_loss: 0.3525 | test_acc: 0.9688\n",
            "Epoch: 7 | train_loss: 0.4246 | train_acc: 0.8771 | test_loss: 0.3263 | test_acc: 0.9563\n",
            "Epoch: 8 | train_loss: 0.3885 | train_acc: 0.8979 | test_loss: 0.3465 | test_acc: 0.9443\n",
            "Epoch: 9 | train_loss: 0.3795 | train_acc: 0.8812 | test_loss: 0.3127 | test_acc: 0.9193\n",
            "Epoch: 10 | train_loss: 0.3752 | train_acc: 0.8688 | test_loss: 0.2811 | test_acc: 0.9625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import utils\n",
        "\n",
        "# Save the model\n",
        "utils.save_model(model=effnetb2,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
      ],
      "metadata": {
        "id": "FwCRnDX5Aeuo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa39d71e-1599-429d-aff6-066b36c47267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saving model to: models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to figure out teh size of this!!\n",
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size# division converts bytes to megabytes (roughly)\n",
        "print(f\"Pretrained EffNetB2 feature extractor model size: {pretrained_effnetb2_model_size} Bytes\")\n",
        "print(f\"Pretrained EffNetB2 feature extractor model size: {pretrained_effnetb2_model_size//(1024*1024)} MBytes\")"
      ],
      "metadata": {
        "id": "MwVb0HYeA78r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b60f4739-74ea-48b9-e97f-9e2920bca7bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained EffNetB2 feature extractor model size: 31314554 Bytes\n",
            "Pretrained EffNetB2 feature extractor model size: 29 MBytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of parameters in EffNetB2\n",
        "effnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\n",
        "effnetb2_total_params"
      ],
      "metadata": {
        "id": "bpXbbAYPBGGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae5b3381-ad6a-4a1c-a464-f226b28c21f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7705221"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary with EffNetB2 statistics\n",
        "effnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],\n",
        "                  \"test_acc\": effnetb2_results[\"test_acc\"][-1],\n",
        "                  \"number_of_parameters\": effnetb2_total_params,\n",
        "                  \"model_size (MB)\": pretrained_effnetb2_model_size}\n",
        "effnetb2_stats"
      ],
      "metadata": {
        "id": "Z5-J_dnTBcnH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af715bc-89e4-4014-b68b-2ab5c25a4d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 0.28108686208724976,\n",
              " 'test_acc': 0.9625,\n",
              " 'number_of_parameters': 7705221,\n",
              " 'model_size (MB)': 31314554}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get all test data paths\n",
        "print(f\"[INFO] Finding all filepaths ending with '.jpg' in directory: {test_dir}\")\n",
        "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "test_data_paths[:5]"
      ],
      "metadata": {
        "id": "F919kx8sBe7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f25536b4-0e0c-46aa-8733-9e53a95ea440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Finding all filepaths ending with '.jpg' in directory: data/pizza_steak_sushi_20_percent/test\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('data/pizza_steak_sushi_20_percent/test/steak/39461.jpg'),\n",
              " PosixPath('data/pizza_steak_sushi_20_percent/test/steak/219196.jpg'),\n",
              " PosixPath('data/pizza_steak_sushi_20_percent/test/steak/1882831.jpg'),\n",
              " PosixPath('data/pizza_steak_sushi_20_percent/test/steak/1335842.jpg'),\n",
              " PosixPath('data/pizza_steak_sushi_20_percent/test/steak/100274.jpg')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note that we want to use CPU for the device, because not everyone will have a GPU and its better as it gives the worst case time!"
      ],
      "metadata": {
        "id": "i1xFhs_jBp6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put EffNetB2 on CPU\n",
        "effnetb2.to(\"cpu\")\n",
        "\n",
        "# Check the device\n",
        "next(iter(effnetb2.parameters())).device"
      ],
      "metadata": {
        "id": "02x0dHCvBhp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73af53e2-a8ad-4c6f-bae5-e6dd963ced2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from timeit import default_timer as timer"
      ],
      "metadata": {
        "id": "tepBLXCkCOBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a predict function!! To for the input of the demo app!\n",
        "# Accepts an image and returns a label and time as well!\n",
        "# Take note that input will have to be transformed to suit the needs of the model!\n",
        "\n",
        "from PIL import Image\n",
        "from timeit import default_timer as timer\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
        "    \"\"\"\n",
        "    # Start the timer\n",
        "    start_time = timer()\n",
        "\n",
        "    # Transform the target image and add a batch dimension\n",
        "    img = effnetb2_transforms(img).unsqueeze(0)\n",
        "\n",
        "    # Put model into evaluation mode and turn on inference mode\n",
        "    effnetb2.eval()\n",
        "    with torch.inference_mode():\n",
        "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
        "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
        "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "    # Calculate the prediction time\n",
        "    pred_time = round(timer() - start_time, 5)\n",
        "\n",
        "    # Return the prediction dictionary and prediction time\n",
        "    return pred_labels_and_probs, pred_time"
      ],
      "metadata": {
        "id": "p0YbEinwBzRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# Get a list of all test image filepaths\n",
        "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "\n",
        "# Randomly select a test image path\n",
        "random_image_path = random.sample(test_data_paths, k=1)[0]\n",
        "\n",
        "# Open the target image\n",
        "image = Image.open(random_image_path)\n",
        "print(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")\n",
        "\n",
        "# Predict on the target image and print out the outputs\n",
        "pred_dict, pred_time = predict(img=image)\n",
        "print(f\"Prediction label and probability dictionary: \\n{pred_dict}\")\n",
        "print(f\"Prediction time: {pred_time} seconds\")"
      ],
      "metadata": {
        "id": "XEBDxV-OCSBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13b0c67c-f6c6-4bf6-dccd-203831c59f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Predicting on image at path: data/pizza_steak_sushi_20_percent/test/pizza/61656.jpg\n",
            "\n",
            "Prediction label and probability dictionary: \n",
            "{'pizza': 0.6250331401824951, 'steak': 0.3042389154434204, 'sushi': 0.07072792947292328}\n",
            "Prediction time: 0.22227 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of example inputs to our Gradio demo\n",
        "example_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]\n",
        "example_list"
      ],
      "metadata": {
        "id": "7O3Z6_UiCTqk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ae5f9bb-35d0-4032-ab8b-5062f64bf5ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['data/pizza_steak_sushi_20_percent/test/pizza/796922.jpg'],\n",
              " ['data/pizza_steak_sushi_20_percent/test/pizza/138961.jpg'],\n",
              " ['data/pizza_steak_sushi_20_percent/test/pizza/2901001.jpg']]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import/install Gradio\n",
        "try:\n",
        "    import gradio as gr\n",
        "except:\n",
        "    !pip -q install gradio\n",
        "    import gradio as gr\n",
        "\n",
        "print(f\"Gradio version: {gr.__version__}\")"
      ],
      "metadata": {
        "id": "aZvaxI9SCl7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fce3053-e526-4776-9849-a0a0b2e61dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mGradio version: 4.36.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create title, description and article strings\n",
        "title = \"FoodVision Mini 🍕🥩🍣\"\n",
        "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\n",
        "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
        "\n",
        "# Create the Gradio demo\n",
        "demo = gr.Interface(fn=predict, # mapping function from input to output\n",
        "                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n",
        "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n",
        "                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n",
        "                    examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article)\n",
        "\n",
        "# Launch the demo!\n",
        "demo.launch(debug=False, # print errors locally?\n",
        "            share=True) # generate a publically shareable URL?"
      ],
      "metadata": {
        "id": "ymIXOptpChSH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "outputId": "c6b5f599-4513-480b-eea7-851476d99269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://7471fcae96ca046c4e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7471fcae96ca046c4e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Create FoodVision mini demo path\n",
        "foodvision_mini_demo_path = Path(\"demos/foodvision_mini/\")\n",
        "\n",
        "# Remove files that might already exist there and create new directory\n",
        "if foodvision_mini_demo_path.exists():\n",
        "    shutil.rmtree(foodvision_mini_demo_path)\n",
        "    foodvision_mini_demo_path.mkdir(parents=True, # make the parent folders?\n",
        "                                    exist_ok=True) # create it even if it already exists?\n",
        "else:\n",
        "    # If the file doesn't exist, create it anyway\n",
        "    foodvision_mini_demo_path.mkdir(parents=True,\n",
        "                                    exist_ok=True)\n",
        "\n",
        "# Check what's in the folder\n",
        "!ls demos/foodvision_mini/"
      ],
      "metadata": {
        "id": "sBuuGedICig4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Create an examples directory\n",
        "foodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\"\n",
        "foodvision_mini_examples_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2. Collect three random test dataset image paths\n",
        "foodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),\n",
        "                            Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),\n",
        "                            Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]\n",
        "\n",
        "# 3. Copy the three random images to the examples directory\n",
        "for example in foodvision_mini_examples:\n",
        "    destination = foodvision_mini_examples_path / example.name\n",
        "    print(f\"[INFO] Copying {example} to {destination}\")\n",
        "    shutil.copy2(src=example, dst=destination)"
      ],
      "metadata": {
        "id": "YsLanecA0OYV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10bca9fc-9f20-4e9f-a5a4-76b201e58ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Copying data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg to demos/foodvision_mini/examples/592799.jpg\n",
            "[INFO] Copying data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg to demos/foodvision_mini/examples/3622237.jpg\n",
            "[INFO] Copying data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg to demos/foodvision_mini/examples/2582289.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for example in os.listdir(foodvision_mini_examples_path):\n",
        "  print(example)"
      ],
      "metadata": {
        "id": "5uFUU64W1DqA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d6196b2-fe41-429d-8100-878b681cdb61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "592799.jpg\n",
            "3622237.jpg\n",
            "2582289.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Create a source path for our target model\n",
        "effnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n",
        "\n",
        "# Create a destination path for our target model\n",
        "# .split is a python thing which will allow give a list of string split by the / and then you get the first index!\n",
        "effnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n",
        "print(effnetb2_foodvision_mini_model_destination)\n",
        "\n",
        "# Whenever we move or try to download things we have to always use try and except!!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AWDHo1Vk1tlB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72105671-e811-4fef-cef7-e1ec77fcd010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "demos/foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Create a source path for our target model\n",
        "effnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n",
        "\n",
        "# Create a destination path for our target model\n",
        "effnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n",
        "\n",
        "# Try to move the file\n",
        "try:\n",
        "    print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")\n",
        "\n",
        "    # Move the model\n",
        "    shutil.move(src=effnetb2_foodvision_mini_model_path,\n",
        "                dst=effnetb2_foodvision_mini_model_destination)\n",
        "\n",
        "    print(f\"[INFO] Model move complete.\")\n",
        "\n",
        "# If the model has already been moved, check if it exists\n",
        "except:\n",
        "    print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved?\")\n",
        "    print(f\"[INFO] Model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\")"
      ],
      "metadata": {
        "id": "BhPPSgzr19m6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4f88199-ee77-4258-c6b7-31f894fcac70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Attempting to move models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth to demos/foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n",
            "[INFO] Model move complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we will try to load the state_dict saved and load it into a model!\n",
        "\n",
        "# But first we have to instantiate the model Eff2b"
      ],
      "metadata": {
        "id": "7vDEWtXd22SL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile script.py\n",
        "# # This above keyword is very important, because we have to turn the model in a python script to be placed into the hugging space!!\n",
        "# def hello():\n",
        "#     print(\"Hello, world!\")\n",
        "\n",
        "# hello()\n"
      ],
      "metadata": {
        "id": "a_ushiWQ2mwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# So here we are creating the model and placing it into a python file!! Whioch is in the demos folder!\n",
        "# We must follow the structure!\n",
        "\n",
        "%%writefile demos/foodvision_mini/model.py\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def create_effnetb2_model(num_classes:int=3,\n",
        "                          seed:int=42):\n",
        "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int, optional): number of classes in the classifier head.\n",
        "            Defaults to 3.\n",
        "        seed (int, optional): random seed value. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module): EffNetB2 feature extractor model.\n",
        "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
        "    \"\"\"\n",
        "    # Create EffNetB2 pretrained weights, transforms and model\n",
        "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "    # Freeze all layers in base model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Change classifier head with random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.3, inplace=True),\n",
        "        nn.Linear(in_features=1408, out_features=num_classes),\n",
        "    )\n",
        "\n",
        "    return model, transforms"
      ],
      "metadata": {
        "id": "jxI7S0wP2lTA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f9b1fe4-e9cb-40eb-ae9f-08c52be062e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/foodvision_mini/model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_mini/app.py\n",
        "### 1. Imports and class names setup ###\n",
        "import gradio as gr\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from model import create_effnetb2_model\n",
        "from timeit import default_timer as timer\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# Setup class names\n",
        "class_names = [\"pizza\", \"steak\", \"sushi\"]\n",
        "\n",
        "### 2. Model and transforms preparation ###\n",
        "\n",
        "# Create EffNetB2 model\n",
        "effnetb2, effnetb2_transforms = create_effnetb2_model(\n",
        "    num_classes=3, # len(class_names) would also work\n",
        ")\n",
        "\n",
        "# Load saved weights\n",
        "effnetb2.load_state_dict(\n",
        "    torch.load(\n",
        "        f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",\n",
        "        map_location=torch.device(\"cpu\"),  # load to CPU\n",
        "    )\n",
        ")\n",
        "\n",
        "### 3. Predict function ###\n",
        "\n",
        "# Create predict function\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
        "    \"\"\"\n",
        "    # Start the timer\n",
        "    start_time = timer()\n",
        "\n",
        "    # Transform the target image and add a batch dimension\n",
        "    img = effnetb2_transforms(img).unsqueeze(0)\n",
        "\n",
        "    # Put model into evaluation mode and turn on inference mode\n",
        "    effnetb2.eval()\n",
        "    with torch.inference_mode():\n",
        "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
        "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
        "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "    # Calculate the prediction time\n",
        "    pred_time = round(timer() - start_time, 5)\n",
        "\n",
        "    # Return the prediction dictionary and prediction time\n",
        "    return pred_labels_and_probs, pred_time\n",
        "\n",
        "### 4. Gradio app ###\n",
        "\n",
        "# Create title, description and article strings\n",
        "title = \"FoodVision Mini 🍕🥩🍣\"\n",
        "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\n",
        "article = \"LJY\"\n",
        "\n",
        "# Create examples list from \"examples/\" directory\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
        "\n",
        "# Create the Gradio demo\n",
        "demo = gr.Interface(fn=predict, # mapping function from input to output\n",
        "                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n",
        "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n",
        "                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n",
        "                    # Create examples list from \"examples/\" directory\n",
        "                    examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article)\n",
        "\n",
        "# Launch the demo!\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "sb5l4SWh34j1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015df7dc-ae6f-4aaf-fc8b-641455255444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/foodvision_mini/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"torch version: {torch.__version__}\")\n",
        "print(f\"torchvision version: {torchvision.__version__}\")\n",
        "print(f\"Gradio version: {gr.__version__}\")"
      ],
      "metadata": {
        "id": "Kvn4JZl948tz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "192dc949-246a-4be8-ac00-69b6dbfdc54d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.3.0+cu121\n",
            "torchvision version: 0.18.0+cu121\n",
            "Gradio version: 4.36.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_mini/requirements.txt\n",
        "torch==2.3.0+cu121\n",
        "torchvision==0.18.0+cu121\n",
        "gradio==4.36.0\n"
      ],
      "metadata": {
        "id": "tjTl228y5So4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e883deaa-a8b6-4301-bbec-70570901df90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/foodvision_mini/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls demos/foodvision_mini"
      ],
      "metadata": {
        "id": "Z7Zp6dfO5cCG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4550664d-676f-483d-854b-0062a13f88a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth  model.py\n",
            "app.py\t\t\t\t\t\t\t\t\t   requirements.txt\n",
            "examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change into and then zip the foodvision_mini folder but exclude certain files\n",
        "!cd demos/foodvision_mini && zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
        "\n",
        "# Download the zipped FoodVision Mini app (if running in Google Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(\"demos/foodvision_mini.zip\")\n",
        "except:\n",
        "    print(\"Not running in Google Colab, can't use google.colab.files.download(), please manually download.\")"
      ],
      "metadata": {
        "id": "v9YPEmYO5lVU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "0945236f-b73e-479f-90db-7f1b41827160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth (deflated 8%)\n",
            "  adding: app.py (deflated 57%)\n",
            "  adding: examples/ (stored 0%)\n",
            "  adding: examples/592799.jpg (deflated 1%)\n",
            "  adding: examples/3622237.jpg (deflated 0%)\n",
            "  adding: examples/2582289.jpg (deflated 17%)\n",
            "  adding: model.py (deflated 55%)\n",
            "  adding: requirements.txt (deflated 13%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2c366ae4-ac5c-417a-b8ff-95b70a19ed15\", \"foodvision_mini.zip\", 28977666)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KXZIFqS157Si"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}